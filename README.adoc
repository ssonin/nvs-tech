= NVS Tech Search API
:toc:
:toc-placement!:

image:https://img.shields.io/badge/vert.x-5.0.6-purple.svg[link="https://vertx.io"]
image:https://img.shields.io/badge/java-21-blue.svg[link="https://openjdk.org/"]
image:https://img.shields.io/badge/postgresql-16-blue.svg[link="https://www.postgresql.org/"]

A REST API for managing clients and documents with advanced full-text search capabilities powered by PostgreSQL.

toc::[]

== Overview

This application provides a searchable repository of clients and their associated documents. The search functionality leverages PostgreSQL's full-text search with custom thesaurus support for synonym matching.

=== Key Features

* *Client Management*: Create and retrieve client records with contact information
* *Document Management*: Attach documents to clients with searchable content
* *Advanced Search*:
** Find clients by name, email, or description
** Search documents with synonym support (e.g., "address proof" matches "utility bill")
** Unified search across both clients and documents with relevance ranking
* *Full-Text Search*: PostgreSQL-powered with custom thesaurus for domain-specific synonyms

=== Technology Stack

* *Runtime*: Java 21
* *Framework*: Vert.x 5.0.6 (Web, Native PostgreSQL Client)
* *Database*: PostgreSQL 16 with full-text search
* *Migrations*: Flyway 11.7.2
* *Build*: Gradle 8.11 with Shadow plugin
* *Containerisation*: Docker with multi-stage builds
* *Testing*: JUnit 5 with Testcontainers (for integration tests)

==== Why Vert.x + PostgreSQL?

This stack is chosen for its exceptional performance and efficiency:

* *Performance*: Vert.x with PostgreSQL consistently ranks among the fastest Java frameworks in link:https://www.techempower.com/benchmarks/#section=data-r23&test=fortune[TechEmpower benchmarks]
* *Native PostgreSQL Driver*: Vert.x uses a reactive, non-blocking PostgreSQL client (not JDBC)
* *SQL Pipelining*: Batch multiple queries efficiently without round-trip overhead
* *Asynchronous Core*: Event-loop based architecture provides better scalability than traditional blocking I/O
* *Resource Efficiency*: Handles thousands of concurrent connections with minimal threads

This makes it ideal for I/O-intensive applications like search APIs where database interaction is the primary bottleneck.

== Getting Started

=== Prerequisites

* Docker and Docker Compose
* (Optional) Java 21 and Gradle 8+ for local development

=== Quick Start

1. Clone the repository:
+
[source,bash]
----
git clone <repository-url>
cd nvs-tech
----

2. Start the application:
+
[source,bash]
----
docker compose up --build
----
+
This will:

* Build and start PostgreSQL 16 with custom thesaurus configuration
* Apply database migrations automatically via Flyway
* Build and start the application on port 8888

3. Verify the application is running:
+
[source,bash]
----
curl "http://localhost:8888/api/v1/search?q=test"
----
This will return `200 OK` with an empty array `[]` if no data exists, which confirms the app is running.

4. Stop the application:
+
[source,bash]
----
docker compose down
----
+
To remove volumes as well:
+
[source,bash]
----
docker compose down -v
----

5. Clean rebuild and restart (stops containers, removes volumes/data, rebuilds images, and starts fresh):
+
[source,bash]
----
docker compose down -v && docker compose up --build
----

=== Configuration

Environment variables (with defaults):

[cols="1,1,3"]
|===
|Variable |Default |Description

|`PGHOST`
|`postgres`
|PostgreSQL host

|`PGPORT`
|`5432`
|PostgreSQL port

|`PGDATABASE`
|`nvs_tech`
|Database name

|`PGUSER`
|`nvs_tech`
|Database user

|`PGPASSWORD`
|`nvs_tech`
|Database password

|`HTTP_PORT`
|`8888`
|HTTP server port
|===

Override in `docker-compose.yml` or create a `.env` file.

== API Documentation

Full OpenAPI 3.1 specification available in link:src/main/resources/openapi.yml[openapi.yml].

=== Endpoints

==== Create Client

[source,bash]
----
POST /api/v1/clients
Content-Type: application/json

{
  "first_name": "Chandler",
  "last_name": "Bing",
  "email": "chandler.bing@neviswealth.com",
  "description": "Sarcastic, self-deprecating office worker with a sharp sense of humor, known for cracking jokes to deflect awkward situations."
}
----

*Response*: `201 Created`
[source,json]
----
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "created_at": "2024-12-22T10:30:00Z",
  "first_name": "Chandler",
  "last_name": "Bing",
  "email": "chandler.bing@neviswealth.com",
  "description": "Sarcastic, self-deprecating office worker with a sharp sense of humor, known for cracking jokes to deflect awkward situations."
}
----

==== Get Client

[source,bash]
----
GET /api/v1/clients/{id}
----

*Response*: `200 OK` (same schema as create response)

==== Create Document

[source,bash]
----
POST /api/v1/clients/{client_id}/documents
Content-Type: application/json

{
  "title": "Chandler Bing's Utility Bill of Awkwardness",
  "content": "This official-looking utility bill details the excessive energy I've wasted trying to explain my job to my parents and the high emotional charges from every failed relationship since Janice. It also includes a surprise late fee for that one time I accidentally proposed, because apparently sarcasm doesn't show up on the meter. Could this BE any more expensive?"
}
----

*Response*: `201 Created`
[source,json]
----
{
  "id": "660e8400-e29b-41d4-a716-446655440001",
  "created_at": "2024-12-22T10:35:00Z",
  "client_id": "550e8400-e29b-41d4-a716-446655440000",
  "title": "Chandler Bing's Utility Bill of Awkwardness",
  "content": "This official-looking utility bill details the excessive energy I've wasted trying to explain my job to my parents and the high emotional charges from every failed relationship since Janice. It also includes a surprise late fee for that one time I accidentally proposed, because apparently sarcasm doesn't show up on the meter. Could this BE any more expensive?"
}
----

==== Search

[source,bash]
----
GET /api/v1/search?q={query}
----

*Response*: `200 OK`
[source,json]
----
[
  {
    "type": "client",
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "created_at": "2024-12-22T10:30:00Z",
    "first_name": "Chandler",
    "last_name": "Bing",
    "email": "chandler.bing@neviswealth.com",
    "description": "Sarcastic, self-deprecating office worker...",
    "rank": 0.607927
  },
  {
    "type": "document",
    "id": "660e8400-e29b-41d4-a716-446655440001",
    "created_at": "2024-12-22T10:35:00Z",
    "client_id": "550e8400-e29b-41d4-a716-446655440000",
    "title": "Chandler Bing's Utility Bill of Awkwardness",
    "content": "This official-looking utility bill details...",
    "rank": 0.456234
  }
]
----

Results are sorted by relevance (rank) in descending order.

== Example Usage

Using the example client and document from the API documentation above, here are various search scenarios demonstrating the capabilities:

=== Direct Term Match

Search for documents containing "utility bill":

[source,bash]
----
curl "http://localhost:8888/api/v1/search?q=utility+bill"
----

Returns Chandler's document as a direct match on "utility bill" in the content.

=== Synonym Match (Thesaurus)

Search for "address proof":

[source,bash]
----
curl "http://localhost:8888/api/v1/search?q=address+proof"
----

Returns Chandler's document even though it doesn't contain "address proof" - the custom thesaurus maps this to "utility bill".

=== Name-Based Search

Search across both resources by name:

[source,bash]
----
curl "http://localhost:8888/api/v1/search?q=chandler"
----

Returns:

* *Client*: Matches by first name
* *Document*: Matches by title ("Chandler Bing's Utility Bill...")

=== Content and Description Search

Search for "awkward":

[source,bash]
----
curl "http://localhost:8888/api/v1/search?q=awkward"
----

Returns:

* *Client*: Matches by description ("...deflect awkward situations")
* *Document*: Matches by content ("...Utility Bill of Awkwardness")

=== Email Domain Search

Search for clients by company:

[source,bash]
----
curl "http://localhost:8888/api/v1/search?q=NevisWealth"
----

Returns Chandler's client record by matching the email domain (`chandler.bing@neviswealth.com`). The email field is tokenised by splitting on special characters (`@`, `.`, `_`, `+`, `-`).

=== Complete Workflow Example

[source,bash]
----
# 1. Create a client
CLIENT_ID=$(curl -s -X POST http://localhost:8888/api/v1/clients \
  -H "Content-Type: application/json" \
  -d '{
    "first_name": "Chandler",
    "last_name": "Bing",
    "email": "chandler.bing@neviswealth.com",
    "description": "Sarcastic, self-deprecating office worker with a sharp sense of humor, known for cracking jokes to deflect awkward situations."
  }' | jq -r '.id')

# 2. Add a document
curl -X POST "http://localhost:8888/api/v1/clients/${CLIENT_ID}/documents" \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Chandler Bing'\''s Utility Bill of Awkwardness",
    "content": "This official-looking utility bill details the excessive energy I'\''ve wasted trying to explain my job to my parents and the high emotional charges from every failed relationship since Janice. It also includes a surprise late fee for that one time I accidentally proposed, because apparently sarcasm doesn'\''t show up on the meter. Could this BE any more expensive?"
  }'

# 3. Search by direct term
curl "http://localhost:8888/api/v1/search?q=utility+bill"

# 4. Search by synonym
curl "http://localhost:8888/api/v1/search?q=address+proof"

# 5. Search by name
curl "http://localhost:8888/api/v1/search?q=chandler"

# 6. Retrieve client details
curl "http://localhost:8888/api/v1/clients/${CLIENT_ID}"
----

== Testing with Postman

A Postman collection with example requests is available at link:src/test/resources/postman/nvs-tech.postman_collection.json[nvs-tech.postman_collection.json].

To use:

1. Import the collection into Postman
2. Ensure the application is running (`docker compose up --build`)
3. Run requests or the entire collection

The collection includes:
* All CRUD operations
* Various search scenarios
* Edge cases (404s, validation errors)

== Architecture & Design Decisions

=== Full-Text Search Implementation

The application uses PostgreSQL's native full-text search capabilities with several optimisations:

==== Text Search Vectors

Pre-computed `tsvector` columns are generated automatically using PostgreSQL's `GENERATED ALWAYS AS` syntax. This provides:

* *Zero maintenance overhead*: Vectors update automatically on INSERT
* *Query performance*: Search operations query pre-computed vectors, not raw text
* *Weighted ranking*: Different fields receive different importance weights

For clients:
[source,sql]
----
search tsvector GENERATED ALWAYS AS (
  setweight(to_tsvector('english', coalesce(first_name, '')), 'A') ||
  setweight(to_tsvector('english', coalesce(last_name, '')), 'A') ||
  setweight(to_tsvector('english', regexp_replace(coalesce(email, ''), '[@._+\-]+', ' ', 'g')), 'B') ||
  setweight(to_tsvector('english', coalesce(description, '')), 'C')
) STORED
----

Weight priorities: `A` (name) > `B` (email) > `C` (description)

==== Email Tokenisation

Email addresses are tokenised by replacing special characters with spaces:
[source,sql]
----
regexp_replace(email, '[@._+\-]+', ' ', 'g')
----

This allows searching by domain (e.g., "NevisWealth") or username components.

==== Custom Thesaurus

A domain-specific link:src/main/resources/db/tsearch_data/custom_thesaurus.ths[custom_thesaurus.ths] maps business terminology to synonyms:

[source,text]
----
address proof : address_proof
utility bill  : address_proof
----

The thesaurus is configured as the first dictionary in the English text search configuration, checked before stemming. This enables synonym expansion while preserving standard language processing.

Configuration (link:src/main/resources/db/migration/V001__configure_text_search.sql[V001++__++configure_text_search.sql]):
[source,sql]
----
CREATE TEXT SEARCH DICTIONARY custom_thesaurus (
  TEMPLATE = thesaurus,
  DictFile = 'custom_thesaurus',
  Dictionary = pg_catalog.simple
);

ALTER TEXT SEARCH CONFIGURATION english
  ALTER MAPPING FOR asciiword
    WITH custom_thesaurus, english_stem;
----

==== GIN Indexes

Generalised Inverted Indexes provide fast full-text lookups:
[source,sql]
----
CREATE INDEX clients_search_idx ON clients USING GIN (search);
CREATE INDEX documents_search_idx ON documents USING GIN (search);
----

==== Query Strategy

The search endpoint:

1. Executes parallel queries against both `clients` and `documents` tables
2. Uses `plainto_tsquery` to parse the search string into a query
3. Ranks results using `ts_rank` for relevance scoring
4. Merges and sorts results by rank in the application layer

[source,java]
----
Future.all(searchClients(values), searchDocuments(values))
  .map(composite -> {
    final JsonArray clients = composite.resultAt(0);
    final JsonArray documents = composite.resultAt(1);
    return Stream.concat(clients.stream(), documents.stream())
      .map(it -> (JsonObject) it)
      .sorted(comparingDouble(it -> it.getDouble("rank")).reversed())
      .toList();
  })
----

=== Why PostgreSQL Full-Text Search?

For this use case, PostgreSQL full-text search provides:

* *Excellent performance*: Sub-millisecond queries with proper indexing
* *Built-in relevance ranking*: `ts_rank` and `ts_rank_cd` algorithms
* *Powerful synonym support*: Thesaurus and dictionary configuration
* *No additional infrastructure*: No need for Elasticsearch, Solr, or external services
* *ACID guarantees*: Data consistency with transactional updates
* *Simpler deployment*: Single database container vs. multi-component stack
* *Lower operational overhead*: Fewer moving parts to monitor and maintain

Trade-offs:

* Not suitable for larger datasets (>10M documents)
* Less sophisticated ranking algorithms than dedicated search engines
* Limited real-time update capabilities compared to specialised solutions
* Fewer analysis features (no highlighting, faceting, or advanced analysers)

For a focused demo project with moderate data volumes, PostgreSQL provides the best balance of features, performance, and simplicity.

=== Vert.x Architecture

The application follows Vert.x best practices with a verticle-based architecture:

==== Verticles

* *App*: Main verticle that orchestrates startup, runs Flyway migrations, and deploys other verticles
* *ApiVerticle*: HTTP layer handling routing, request/response formatting, and error handling
* *RepositoryVerticle*: Data access layer managing PostgreSQL interactions

==== Event Bus Communication

Verticles communicate via Vert.x's distributed event bus:

[source,java]
----
vertx.eventBus()
  .<JsonObject>request("clients.create", payload)
  .onSuccess(reply -> ...)
  .onFailure(ctx::fail);
----

Benefits:

* *Decoupling*: API and repository layers are independent
* *Testability*: Easy to mock event bus for unit tests
* *Scalability*: Event bus supports clustering (though not needed here)

==== Reactive PostgreSQL Client

Uses Vert.x's native reactive PostgreSQL client:

[source,java]
----
pool.withConnection(conn ->
  conn.preparedQuery(insertClient())
    .execute(values)
    .map(rows -> clientFromRow(rows.iterator().next()))
)
----

Advantages over JDBC:

* *Non-blocking I/O*: Doesn't tie up threads waiting for database responses
* *Connection pooling*: Built-in reactive pool management
* *Prepared statements*: Native protocol-level preparation
* *Pipeline support*: Batch multiple commands in a single network round-trip

=== Database Schema

The schema uses several PostgreSQL features:

* *Generated columns*: Automatic tsvector maintenance
* *Partial unique indexes*: `UNIQUE (lower(email)) WHERE state = 'ACTIVE'`
* *Soft deletes*: `state` column enables logical deletion (prepared for future use)
* *Timestamps*: Automatic `created_at` and `updated_at` tracking

See migrations in link:src/main/resources/db/migration/[src/main/resources/db/migration/]:

* link:src/main/resources/db/migration/V001__configure_text_search.sql[V001]: Text search configuration
* link:src/main/resources/db/migration/V002__create_clients_table.sql[V002]: Clients table with search vectors
* link:src/main/resources/db/migration/V003__create_documents_table.sql[V003]: Documents table with search vectors

== Development

=== Local Development

1. Start PostgreSQL:
+
[source,bash]
----
docker compose up --build postgres
----

2. Package the application:
+
[source,bash]
----
./gradlew clean assemble
----

3. Run the application:
+
[source,bash]
----
./gradlew clean run
----

=== Running Tests

[source,bash]
----
./gradlew clean test
----

Integration tests use Testcontainers to spin up a PostgreSQL instance automatically, ensuring tests run against the actual database with full-text search configured.

=== Building Fat JAR

[source,bash]
----
./gradlew clean shadowJar
----

The fat JAR will be available at `build/libs/nvs-tech-1.0.0-SNAPSHOT-fat.jar`.

Run it directly:
[source,bash]
----
java -jar build/libs/nvs-tech-1.0.0-SNAPSHOT-fat.jar
----

== Known Limitations & Future Enhancements

=== Current Limitations

* *No document content summarisation*: The optional feature has not been implemented
* *Limited error messages*: Error responses are basic and could be more descriptive
* *No pagination*: Search results return all matches (acceptable for small datasets)
* *No authentication*: Endpoints are publicly accessible
* *Case-insensitive search only*: PostgreSQL text search normalises all input

=== Potential Enhancements

* *Document Summarisation*: LLM-based content summarisation using Claude API
* *Advanced Ranking*: Custom boost factors, phrase matching, proximity scoring
* *Result Highlighting*: Return matched snippets with search terms highlighted
* *Faceted Search*: Filter by client, date range, document type
* *Pagination*: Limit and offset parameters for large result sets
* *Fuzzy Matching*: Typo tolerance using trigram similarity
* *Audit Trail*: Track document changes and access patterns
* *Rate Limiting*: Protect against abuse
* *Authentication & Authorisation*: JWT-based API security
* *Expanded Thesaurus*: More comprehensive synonym mappings for financial/legal domains

== Troubleshooting

=== Application won't start

Check logs:
[source,bash]
----
docker compose logs app
----

Ensure PostgreSQL is healthy:
[source,bash]
----
docker compose ps postgres
----

=== Search returns no results

Verify data exists:
[source,bash]
----
docker exec -it nvs-tech-postgres psql -U nvs_tech -d nvs_tech -c "SELECT COUNT(*) FROM clients;"
----

Check search vector generation:
[source,bash]
----
docker exec -it nvs-tech-postgres psql -U nvs_tech -d nvs_tech -c "SELECT id, search FROM clients LIMIT 1;"
----

Verify thesaurus is loaded:
[source,bash]
----
docker exec -it nvs-tech-postgres psql -U nvs_tech -d nvs_tech -c "SELECT to_tsvector('english', 'address proof');"

docker exec -it nvs-tech-postgres psql -U nvs_tech -d nvs_tech -c "SELECT to_tsvector('english', 'utility bill');"
----

Both commands should return the same token (`'address_proof':1`), confirming the thesaurus correctly maps "utility bill" to "address proof":
[source,text]
----
    to_tsvector
-------------------
 'address_proof':1
(1 row)
----

=== Database migration issues

Reset the database:
[source,bash]
----
docker compose down -v
docker compose up --build
----

== References

* link:https://vertx.io/docs/[Vert.x Documentation]
* link:https://www.techempower.com/benchmarks/#section=data-r23&test=fortune[TechEmpower Benchmarks - Vert.x Performance]
* link:https://www.postgresql.org/docs/16/textsearch.html[PostgreSQL Full-Text Search]
* link:https://www.postgresql.org/docs/16/textsearch-dictionaries.html[PostgreSQL Text Search Dictionaries]
* link:https://swagger.io/specification/[OpenAPI Specification]
* link:https://flywaydb.org/documentation/[Flyway Documentation]
